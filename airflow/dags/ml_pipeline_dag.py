"""
DAG Airflow pour pipeline ML automatis√© de d√©tection de malwares
Ex√©cution quotidienne : Collecte ‚Üí Extraction ‚Üí Entra√Ænement ‚Üí D√©ploiement
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import sys
import os
import logging

# Ajouter le path pour importer le toolkit
sys.path.insert(0, '/opt/airflow/my_ml_toolkit')

from data_loader.binary import BinaryLoader
from feature_extraction.binary_features import BinaryFeatureExtractor
from preprocessing.numeric_prep import NumericPreprocessor
from modeling.auto_trainer import AutoTrainer
import pandas as pd
import pickle
import json

# Configuration
DATA_DIR = '/opt/airflow/data'
MODELS_DIR = '/opt/airflow/models'
MALWARE_DIR = f'{DATA_DIR}/malware_samples'
BENIGN_DIR = f'{DATA_DIR}/benign_samples'

default_args = {
    'owner': 'tatiana',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'ml_malware_detection_pipeline',
    default_args=default_args,
    description='Pipeline automatis√© de d√©tection de malwares',
    schedule_interval='@daily',  # Ex√©cution quotidienne
    catchup=False,
    tags=['ml', 'cybersecurity', 'malware'],
)


def check_data_availability(**context):
    """V√©rifier si de nouvelles donn√©es sont disponibles"""
    logging.info("üîç V√©rification de la disponibilit√© des donn√©es...")
    
    # Cr√©er les dossiers s'ils n'existent pas
    os.makedirs(MALWARE_DIR, exist_ok=True)
    os.makedirs(BENIGN_DIR, exist_ok=True)
    os.makedirs(MODELS_DIR, exist_ok=True)
    
    # Compter les fichiers
    malware_count = len([f for f in os.listdir(MALWARE_DIR) if os.path.isfile(os.path.join(MALWARE_DIR, f))])
    benign_count = len([f for f in os.listdir(BENIGN_DIR) if os.path.isfile(os.path.join(BENIGN_DIR, f))])
    
    logging.info(f"   Malwares trouv√©s: {malware_count}")
    logging.info(f"   Fichiers l√©gitimes trouv√©s: {benign_count}")
    
    # Pousser les infos vers XCom
    context['ti'].xcom_push(key='malware_count', value=malware_count)
    context['ti'].xcom_push(key='benign_count', value=benign_count)
    
    if malware_count < 10 or benign_count < 10:
        logging.warning("‚ö†Ô∏è  Pas assez de donn√©es, g√©n√©ration de donn√©es synth√©tiques...")
        return 'generate_synthetic_data'
    
    return 'extract_features'


def generate_synthetic_data(**context):
    """G√©n√©rer des donn√©es synth√©tiques pour la d√©mo"""
    import numpy as np
    
    logging.info("üîß G√©n√©ration de donn√©es synth√©tiques...")
    
    # G√©n√©rer 50 malwares simul√©s (haute entropie)
    for i in range(50):
        data = bytes(np.random.randint(0, 256, 1000))
        filepath = os.path.join(MALWARE_DIR, f'synthetic_malware_{i}.bin')
        with open(filepath, 'wb') as f:
            f.write(data)
    
    # G√©n√©rer 50 fichiers l√©gitimes simul√©s (basse entropie)
    for i in range(50):
        data = b"MZ" + (b"\x00" * 300) + (b"Normal executable code " * 30)
        filepath = os.path.join(BENIGN_DIR, f'synthetic_benign_{i}.exe')
        with open(filepath, 'wb') as f:
            f.write(data)
    
    logging.info("‚úÖ Donn√©es synth√©tiques g√©n√©r√©es avec succ√®s")


def extract_features(**context):
    """Extraire les features de tous les fichiers"""
    logging.info("üîç Extraction des features...")
    
    loader = BinaryLoader()
    extractor = BinaryFeatureExtractor()
    
    all_features = []
    
    # Charger et extraire features des malwares
    logging.info("   Traitement des malwares...")
    malware_files = loader.load_directory(MALWARE_DIR)
    for filename, data in malware_files:
        features = extractor.extract_all_features(data)
        features['label'] = 1  # Malware
        features['filename'] = filename
        all_features.append(features)
    
    # Charger et extraire features des fichiers l√©gitimes
    logging.info("   Traitement des fichiers l√©gitimes...")
    benign_files = loader.load_directory(BENIGN_DIR)
    for filename, data in benign_files:
        features = extractor.extract_all_features(data)
        features['label'] = 0  # L√©gitime
        features['filename'] = filename
        all_features.append(features)
    
    # Sauvegarder les features
    df = pd.DataFrame(all_features)
    features_path = f'{DATA_DIR}/features.csv'
    df.to_csv(features_path, index=False)
    
    logging.info(f"‚úÖ Features extraites: {len(df)} fichiers, {df.shape[1]} features")
    
    # Pousser le chemin vers XCom
    context['ti'].xcom_push(key='features_path', value=features_path)


def train_model(**context):
    """Entra√Æner le mod√®le de d√©tection"""
    logging.info("ü§ñ Entra√Ænement du mod√®le...")
    
    # R√©cup√©rer le chemin des features
    features_path = context['ti'].xcom_pull(task_ids='extract_features', key='features_path')
    
    # Charger les features
    df = pd.read_csv(features_path)
    
    # Pr√©parer les donn√©es
    X = df.drop(columns=['label', 'md5', 'sha256', 'filename'], errors='ignore')
    y = df['label']
    
    # Pr√©traiter
    preprocessor = NumericPreprocessor(scaling_method='standard')
    X_processed = preprocessor.scale_features(X)
    
    # Entra√Æner
    trainer = AutoTrainer(task_type='classification', random_state=42)
    results = trainer.train_all_models(X_processed, y, verbose=True)
    
    # Sauvegarder le meilleur mod√®le
    best_model_name, best_model = trainer.get_best_model()
    
    model_info = {
        'model_name': best_model_name,
        'accuracy': trainer.best_score,
        'timestamp': datetime.now().isoformat(),
        'n_samples': len(df),
        'n_features': X_processed.shape[1]
    }
    
    # Sauvegarder le mod√®le
    model_path = f'{MODELS_DIR}/malware_detector.pkl'
    with open(model_path, 'wb') as f:
        pickle.dump({
            'model': best_model,
            'preprocessor': preprocessor,
            'feature_columns': list(X.columns),
            'info': model_info
        }, f)
    
    # Sauvegarder les m√©tadonn√©es
    metadata_path = f'{MODELS_DIR}/model_metadata.json'
    with open(metadata_path, 'w') as f:
        json.dump(model_info, f, indent=2)
    
    logging.info(f"‚úÖ Mod√®le entra√Æn√©: {best_model_name}")
    logging.info(f"   Accuracy: {trainer.best_score:.4f}")
    logging.info(f"   Sauvegard√©: {model_path}")
    
    # Pousser les r√©sultats
    context['ti'].xcom_push(key='model_path', value=model_path)
    context['ti'].xcom_push(key='model_info', value=model_info)


def evaluate_model(**context):
    """√âvaluer le mod√®le et g√©n√©rer un rapport"""
    logging.info("üìä √âvaluation du mod√®le...")
    
    model_info = context['ti'].xcom_pull(task_ids='train_model', key='model_info')
    
    # Cr√©er un rapport d'√©valuation
    report = f"""
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    üìä RAPPORT D'ENTRA√éNEMENT - D√âTECTION DE MALWARES
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    üìÖ Date: {model_info['timestamp']}
    
    ü§ñ Mod√®le: {model_info['model_name']}
    
    üìà Performance:
       - Accuracy: {model_info['accuracy']:.4f}
    
    üìä Dataset:
       - Nombre d'√©chantillons: {model_info['n_samples']}
       - Nombre de features: {model_info['n_features']}
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
    
    logging.info(report)
    
    # Sauvegarder le rapport
    report_path = f'{MODELS_DIR}/training_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
    with open(report_path, 'w') as f:
        f.write(report)
    
    logging.info(f"‚úÖ Rapport sauvegard√©: {report_path}")


def deploy_model(**context):
    """D√©ployer le mod√®le (placeholder pour int√©gration BentoML)"""
    logging.info("üöÄ D√©ploiement du mod√®le...")
    
    model_path = context['ti'].xcom_pull(task_ids='train_model', key='model_path')
    
    logging.info(f"   Mod√®le pr√™t pour d√©ploiement: {model_path}")
    logging.info("   TODO: Int√©gration avec BentoML pour API REST")
    
    # Ici on pourrait d√©clencher un build BentoML
    # bentoml build && bentoml containerize
    
    logging.info("‚úÖ Mod√®le d√©ploy√© avec succ√®s")


# ==========================================
# D√©finition des t√¢ches
# ==========================================

check_data = PythonOperator(
    task_id='check_data_availability',
    python_callable=check_data_availability,
    provide_context=True,
    dag=dag,
)

generate_data = PythonOperator(
    task_id='generate_synthetic_data',
    python_callable=generate_synthetic_data,
    provide_context=True,
    dag=dag,
)

extract = PythonOperator(
    task_id='extract_features',
    python_callable=extract_features,
    provide_context=True,
    dag=dag,
)

train = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    provide_context=True,
    dag=dag,
)

evaluate = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model,
    provide_context=True,
    dag=dag,
)

deploy = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_model,
    provide_context=True,
    dag=dag,
)

# ==========================================
# D√©finition du workflow
# ==========================================

check_data >> [generate_data, extract]
generate_data >> extract
extract >> train >> evaluate >> deploy
